{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Identification Demo\n",
    "# Levitador Magn√©tico Benchmark\n",
    "\n",
    "This notebook demonstrates how to use the modular optimization framework to identify parameters of the levitator system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll demonstrate:\n",
    "1. Loading the benchmark problem\n",
    "2. Running individual optimizers\n",
    "3. Comparing multiple algorithms\n",
    "4. Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent) if 'notebooks' in str(Path.cwd()) else '.')\n",
    "\n",
    "from levitador_benchmark import LevitadorBenchmark\n",
    "from src.optimization import (\n",
    "    RandomSearch, DifferentialEvolution, GeneticAlgorithm,\n",
    "    GreyWolfOptimizer, ArtificialBeeColony, HoneyBadgerAlgorithm,\n",
    "    ShrimpOptimizer, TianjiOptimizer\n",
    ")\n",
    "from src.visualization.plots import plot_convergence, plot_comparison_boxplot\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Benchmark Problem\n",
    "\n",
    "The benchmark represents the levitator system with parameters to identify: k0, k, and a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark instance\n",
    "problema = LevitadorBenchmark(random_seed=42, verbose=True)\n",
    "\n",
    "print(f\"\\nProblem dimension: {problema.dim}\")\n",
    "print(f\"Search space bounds: {problema.bounds}\")\n",
    "print(f\"Reference solution: {problema.reference_solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test a Single Optimizer\n",
    "\n",
    "Let's start with Grey Wolf Optimizer as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Grey Wolf Optimizer\n",
    "print(\"Running Grey Wolf Optimizer...\\n\")\n",
    "gwo = GreyWolfOptimizer(problema, pop_size=30, max_iter=50, random_seed=42, verbose=True)\n",
    "best_solution, best_fitness = gwo.optimize()\n",
    "\n",
    "print(f\"\\nüèÜ Best Solution Found:\")\n",
    "print(f\"  k0 = {best_solution[0]:.6f} H\")\n",
    "print(f\"  k  = {best_solution[1]:.6f} H\")\n",
    "print(f\"  a  = {best_solution[2]:.6f} m\")\n",
    "print(f\"\\nFitness (MSE): {best_fitness:.6e}\")\n",
    "print(f\"Evaluations: {gwo.evaluations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gwo.history, linewidth=2, color='blue')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Best Fitness (MSE)', fontsize=12)\n",
    "plt.title('Grey Wolf Optimizer - Convergence', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Multiple Algorithms\n",
    "\n",
    "Let's compare several bio-inspired algorithms on the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers to compare\n",
    "optimizers_to_test = [\n",
    "    ('Random Search', RandomSearch, {'n_iterations': 1500, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Differential Evolution', DifferentialEvolution, {'pop_size': 30, 'max_iter': 50, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Genetic Algorithm', GeneticAlgorithm, {'pop_size': 30, 'generations': 50, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Grey Wolf', GreyWolfOptimizer, {'pop_size': 30, 'max_iter': 50, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Artificial Bee Colony', ArtificialBeeColony, {'pop_size': 30, 'max_iter': 50, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Honey Badger', HoneyBadgerAlgorithm, {'pop_size': 30, 'max_iter': 50, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Shrimp', ShrimpOptimizer, {'pop_size': 30, 'max_iter': 50, 'random_seed': 42, 'verbose': False}),\n",
    "    ('Tianji', TianjiOptimizer, {'pop_size': 30, 'max_iter': 50, 'random_seed': 42, 'verbose': False}),\n",
    "]\n",
    "\n",
    "# Run all optimizers\n",
    "results = {}\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer_class, config in optimizers_to_test:\n",
    "    print(f\"Running {name}...\")\n",
    "    optimizer = optimizer_class(problema, **config)\n",
    "    best_sol, best_fit = optimizer.optimize()\n",
    "    \n",
    "    results[name] = best_fit\n",
    "    histories[name] = optimizer.history\n",
    "    print(f\"  ‚úì Fitness: {best_fit:.6e}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for name in sorted(results.keys(), key=lambda x: results[x]):\n",
    "    print(f\"{name:<25} {results[name]:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize All Convergence Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all convergence curves\n",
    "plot_convergence(histories, title='Algorithm Convergence Comparison', log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Comparison (Multiple Trials)\n",
    "\n",
    "For a more robust comparison, let's run multiple trials of each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple trials\n",
    "n_trials = 10\n",
    "algorithms_to_compare = [\n",
    "    ('DE', DifferentialEvolution, {'pop_size': 30, 'max_iter': 50, 'verbose': False}),\n",
    "    ('Grey Wolf', GreyWolfOptimizer, {'pop_size': 30, 'max_iter': 50, 'verbose': False}),\n",
    "    ('ABC', ArtificialBeeColony, {'pop_size': 30, 'max_iter': 50, 'verbose': False}),\n",
    "    ('Honey Badger', HoneyBadgerAlgorithm, {'pop_size': 30, 'max_iter': 50, 'verbose': False}),\n",
    "]\n",
    "\n",
    "multi_trial_results = {}\n",
    "\n",
    "for name, optimizer_class, config in algorithms_to_compare:\n",
    "    print(f\"Running {name} ({n_trials} trials)...\")\n",
    "    trial_results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Use different seed for each trial\n",
    "        config['random_seed'] = 42 + trial\n",
    "        optimizer = optimizer_class(problema, **config)\n",
    "        _, fitness = optimizer.optimize()\n",
    "        trial_results.append(fitness)\n",
    "    \n",
    "    multi_trial_results[name] = trial_results\n",
    "    mean_fitness = np.mean(trial_results)\n",
    "    std_fitness = np.std(trial_results)\n",
    "    print(f\"  Mean: {mean_fitness:.6e} ¬± {std_fitness:.6e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plot comparison\n",
    "plot_comparison_boxplot(\n",
    "    multi_trial_results,\n",
    "    title=f'Algorithm Comparison ({n_trials} trials)',\n",
    "    log_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Best Solution\n",
    "\n",
    "Let's examine the best solution found and compare it to the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best algorithm from single trial\n",
    "best_algorithm = min(results.keys(), key=lambda x: results[x])\n",
    "print(f\"Best performing algorithm: {best_algorithm}\")\n",
    "print(f\"Fitness: {results[best_algorithm]:.6e}\")\n",
    "\n",
    "# Compare with reference\n",
    "print(f\"\\nReference solution:\")\n",
    "print(f\"  k0 = {problema.reference_solution[0]:.6f} H\")\n",
    "print(f\"  k  = {problema.reference_solution[1]:.6f} H\")\n",
    "print(f\"  a  = {problema.reference_solution[2]:.6f} m\")\n",
    "\n",
    "ref_fitness = problema.fitness_function(problema.reference_solution)\n",
    "print(f\"\\nReference fitness: {ref_fitness:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "Based on the results:\n",
    "\n",
    "1. **Best Performance**: The algorithm with lowest MSE provides the most accurate parameter identification\n",
    "2. **Convergence Speed**: Check which algorithms converge fastest\n",
    "3. **Reliability**: Algorithms with lower standard deviation across trials are more reliable\n",
    "4. **Computational Cost**: Consider evaluations/runtime for practical applications\n",
    "\n",
    "### Using the Framework\n",
    "\n",
    "To use this framework in your own code:\n",
    "\n",
    "```python\n",
    "from levitador_benchmark import LevitadorBenchmark\n",
    "from src.optimization import GreyWolfOptimizer\n",
    "\n",
    "# Create problem\n",
    "problema = LevitadorBenchmark()\n",
    "\n",
    "# Run optimizer\n",
    "optimizer = GreyWolfOptimizer(problema, pop_size=30, max_iter=100)\n",
    "best_solution, best_fitness = optimizer.optimize()\n",
    "\n",
    "print(f\"Solution: {best_solution}\")\n",
    "print(f\"Fitness: {best_fitness}\")\n",
    "```\n",
    "\n",
    "Or use the command-line benchmark runner:\n",
    "\n",
    "```bash\n",
    "python scripts/run_benchmark.py --config config/default.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
